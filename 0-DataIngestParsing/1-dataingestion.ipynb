{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a71978e4",
   "metadata": {},
   "source": [
    "# Import Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8f2477",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import List, Dict, Any\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b2df3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "from langchain.text_splitter import(\n",
    "    RecursiveCharacterTextSplitter,\n",
    "    CharacterTextSplitter,\n",
    "    TokenTextSplitter,\n",
    ")\n",
    "\n",
    "print(\"setup completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0703327e",
   "metadata": {},
   "source": [
    "# Document Structure in LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cfabbd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## create a document, print its content and metadata\n",
    "doc = Document(\n",
    "    page_content=\"this is main text content that is used for processing\",\n",
    "    metadata={\n",
    "        \"source\": \"example.txt\",\n",
    "        \"page\": 1,\n",
    "        \"author\": \"Akhil\",\n",
    "        \"date\": \"2025-08-17\",\n",
    "        \"tags\": [\"data\", \"ingestion\", \"langchain\"],\n",
    "    }\n",
    ")\n",
    "\n",
    "print(f\"document created..!\\ncontent: {doc.page_content}\")\n",
    "print(f\"metadata: {doc.metadata}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d996ba",
   "metadata": {},
   "source": [
    "# Text File Reading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f112a83d",
   "metadata": {},
   "source": [
    "## create files under data/text_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca97a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs(\"data/text_files\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c556f248",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = {\n",
    "    \"data/text_files/python_intro.txt\": \"\"\"Python Programming Introduction\n",
    "Python is a high-level, interpreted programming language known for its readability and versatility. \n",
    "It supports multiple programming paradigms, including procedural, object-oriented, and functional programming. \n",
    "Python's extensive standard library and active community make it a popular choice for web development, data analysis, \n",
    "artificial intelligence, scientific computing, and more.\n",
    "Python's syntax is designed to be clear and straightforward, which makes it an excellent language for beginners.\n",
    "Python supports dynamic typing, meaning that variable types are determined at runtime, allowing for flexibility in coding.\n",
    "Python's extensive libraries and frameworks, such as Django for web development and Pandas for data analysis,\n",
    "make it a powerful tool for developers across various domains.\"\"\",\n",
    "\n",
    "\"data/text_files/machine_learning.txt\": \"\"\"Machine Learning Overview\n",
    "Machine learning is a subset of artificial intelligence that focuses on building systems that can learn from and make predictions based on data.\n",
    "It involves the use of algorithms and statistical models to enable computers to improve their performance on a specific task through experience.\n",
    "Machine learning can be categorized into three main types: supervised learning, unsupervised learning, and  reinforcement learning.\n",
    "Supervised learning involves training a model on labeled data, where the input data is paired with the  correct output.\n",
    "Unsupervised learning involves training a model on unlabeled data, allowing the model to find patterns and relationships in the data without explicit instructions.\n",
    "Reinforcement learning involves training an agent to make decisions by rewarding it for correct actions and penalizing it for incorrect actions.\n",
    "Machine learning is widely used in various applications, including image recognition, natural language processing, and recommendation systems.\"\"\",\n",
    "}\n",
    "\n",
    "for filepath,content in sample_text.items():\n",
    "    with open(filepath, \"w\",encoding=\"utf-8\") as file:\n",
    "        file.write(content)\n",
    "\n",
    "print(\"Sample text files created in 'data/text_files' directory.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c92327f",
   "metadata": {},
   "source": [
    "## TextLoader - Read Single file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96545bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "\n",
    "# load a single text file\n",
    "loader = TextLoader(\"data/text_files/python_intro.txt\", encoding=\"utf-8\")\n",
    "\n",
    "document=loader.load()\n",
    "print(f\"content preview: {document[0].page_content[:100]}...\")  # Display first 100 characters\n",
    "print(f\"metadata: {document[0].metadata}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0033cf68",
   "metadata": {},
   "source": [
    "## DirectoryLoader  - Multiple Text Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "597f345a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import DirectoryLoader\n",
    "\n",
    "directory_loader = DirectoryLoader(\n",
    "    \"data/text_files\",  \n",
    "    glob=\"*.txt\",\n",
    "    loader_cls=TextLoader,\n",
    "    loader_kwargs={\"encoding\": \"utf-8\"},\n",
    "    show_progress=True  \n",
    ")\n",
    "documents = directory_loader.load()\n",
    "\n",
    "print(f\"Number of documents loaded: {len(documents)}\")\n",
    "for i, doc in enumerate(documents):\n",
    "    print(f\"document {i+1}:\")\n",
    "    print(f\"Document source: {doc.metadata[\"source\"]}\")  # Display first 100 characters\n",
    "    print(f\"Document length: {len(doc.page_content)} characters\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c38dbcb9",
   "metadata": {},
   "source": [
    "# Text Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dea3576",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import (\n",
    "    CharacterTextSplitter,\n",
    "    RecursiveCharacterTextSplitter,\n",
    "    TokenTextSplitter\n",
    ")\n",
    "\n",
    "text = documents[0].page_content\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d48d334",
   "metadata": {},
   "source": [
    "## Method 1: Character Splitter \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c4d09e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "char_splitter = CharacterTextSplitter(\n",
    "    separator=\" \",\n",
    "    chunk_size=100,\n",
    "    chunk_overlap=20,\n",
    "    length_function=len\n",
    ")\n",
    "\n",
    "print(\"original data\\n\", text)\n",
    "print(\"------------\")\n",
    "char_hunks = char_splitter.split_text(text)\n",
    "print(f\"created chunks {len(char_hunks)}\")\n",
    "print(\"------------\")\n",
    "print(\"chunk 1\\n\",char_hunks[0])\n",
    "print(\"------------\")\n",
    "print(\"chunk 2\\n\",char_hunks[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a7f265",
   "metadata": {},
   "source": [
    "## Method 2: Recursive Character Splitter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104250e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rec_splitter = RecursiveCharacterTextSplitter(\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"],\n",
    "    chunk_size=100,\n",
    "    chunk_overlap=20,\n",
    "    length_function=len\n",
    ")\n",
    "\n",
    "rec_chunks = rec_splitter.split_text(text)\n",
    "print(f\"created chunks {len(rec_chunks)}\")\n",
    "print(\"------------\")\n",
    "print(f\"chunk 1 \\n {rec_chunks[0]}\")\n",
    "print(\"------------\")\n",
    "print(f\"chunk 2 \\n{rec_chunks[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9613cfb6",
   "metadata": {},
   "source": [
    "## Method 3: Token Splitter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "457befda",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "token_splitter = TokenTextSplitter(\n",
    "    chunk_size=50,\n",
    "    chunk_overlap=10\n",
    ")\n",
    "token_chunks = token_splitter.split_text(text)\n",
    "print(f\"created chunks {len(token_chunks)}\")\n",
    "print(\"------------\")\n",
    "print(f\"chunk 1 \\n {token_chunks[0]}\")\n",
    "print(\"------------\")\n",
    "print(f\"chunk 2 \\n{token_chunks[1]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
